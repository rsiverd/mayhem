# -----------------------------------------------------------------------

* Segments do not need to include the raw data points (i.e., x, y, mag). Those
      are preserved in the _raw_data array and can be recalled using the 'idx'
      values affiliated with each segment.
         --> Keeping the raw values OUT of the segment 'deltas' eliminates the
               need to keep track of which 'seg' columns should be used to
               compute diffs (and which should be ignored). This, in turn,
               simplifies the differences calculation.
     ***** make sure this is true. Maybe those parameters are used in one of
            the routines I didn't play with today.

# -----------------------------------------------------------------------

* Ensure that input data are always at least 2D (even if just a single coord).
      This, in turn, ensures that the all-vs-all 'diffs' is 3D (with datum on
      3rd axis), allowing the same syntax to work in all cases.

# -----------------------------------------------------------------------

* Keep track internally of the kind/units of the different segment properties.
   For 1-D input (e.g., X coords only), segments are length-only. Using
      log-length instead conveniently provides scale factor.

   For 2-D input with two spatial coords (e.g., X,Y or RA,DE), segments
      consist of a length (or log-length) and an orientation angle.

   For 2-D input with one spatial coordinate and one brightness (e.g., X, mag),
      segments consist of length (or log-length) and magnitude difference.

   For 3-D input with X, Y, mag, segments consist of length (or log-length),
      orientation angle, and magnitude difference.

Different segment-listing routines will likely be needed for the different
dimensionalities of input, especially if I hope to continue supporting
magnitude and/or depth cuts.

IDEAS for accommodating these various inputs:
--> Identify 'kind' of data in each column somehow when specifying input
      catalogs. Should check that same kinds given for both lists. Should also 
      store kinds of data separately. 'Kind' in this case is one of either 
      position, flux, or mag. Note that 'flux' can be ~anything linear. 
      Maybe mag is not really needed?
--> When listing segments, 'r_sep' is always calculated (NOTE error in 
      wl_solve_test version ... should compare abs(dx) rather than dx). Need
      to compute r_sep as in the 2-D case always just summing across however
      many columns have been provided.
--> When listing segments, 'angle' is calculated when both X and Y coordinates
      are available. 
--> Option to choose which segment components are produced?? In other words,
      may not want to use mag diffs even if they were provided. Allows user to
      run search more than one way without having to adjust catalogs.

# -----------------------------------------------------------------------

* Extend the utility of this matching system by developing the machinery
   to process input catalogs piece-wise (to avoid exploding memory demands).
   It is not obvious how to make this work ... YES IT IS!

--> All possible segments (subject to cuts applied in listing) from one
catalog must ultimately be differenced from all possible segments of the
other catalog. Making intelligent choices (i.e., restrictions on delta-mag)
can significantly reduce parameter space but not enough to enable handling of
input lists with more than ~40 objects (which isn't that many!).

I think the most sensible improvement will exploit the incremental build-up of
votes in parameter space. Currently in the dither_hist method (best results) we
1) start with empty cumulative segment and object voting spaces
2a) iterate over nudges (dithers) in the histogram bins (the locations of the
      bin edges specifically). For each of these bin arrangements, we do a
      multi-dimensional histogram of the (large) diffs array and identify the
      most highly populated bins. We keep track of the most populated param
      combo from each bin-set for later averaging.
2b) For each 'winning' parameter combination (one per bin-set), votes are cast
      in segment and object space to identify corresponding features. The
      individual votes from each round are added to the cumulative voting
      space from which the most likely correspondences are taken.

Since the cumulative object and segment votes are already constructed in a
piece-wise fashion, there is perhaps room for a divide-and-conquer approach.
The implementation difficulty depends fairly significantly on whether:
* all segments from each catalog can be produced simultaneously (even if their
   all-vs-all differences cannot be stored at once)
* the sheer number of segments alone is too great. In this case, coding
complexity would probably escalate quite rapidly.

...............................

KEY POINT: the 'diffs' array is the only thing that is too large for memory
in most cases. Its dimensions are nsegments1 * nsegments2 whereas the memory
size needed for the segments themselves is just nsegments1 + nsegments2 which
is vastly smaller.


To enable divide-and-conquer, need to change several things:
* diffs array contents will need to be computed more than once (can't be stored)
* want to calculate diffs using a generator routine (with yield) so that
   iteration over all diffs works but that memory is managed.
* can use all diffs by creating a cumulative *histogram* in advance. This
   histogram is then augmented with the histogramdd() results from each diffs
   chunk as it arrives. The histogram size depends on parameter space, not on
   number of segments or diffs. In the current module structure, this change
   effectively involves moving the all_diffs and adj_diffs calculation inside
   the _params_from_hbins() routine. However ...
* In the current module, histograms are created for one nudge_set at a time.
   This avoids spending memory on histograms but would require re-computing
   the all_diffs several times (once per nudge set) which would be slow. A
   better approach is create ALL of the necessary cumulative histograms in
   advance (one for each nudge/adjustment). Then, when iterating over the
   on-the-fly-generated diffs, each diffs chunk is applied to each of the
   nudged histograms. This allows the cumulative histograms to all be built
   with a single pass through the generated diffs (but with more code changes
   needed than if diffs were generated once per nudge set). Maybe implement
   this in second updates pass?
* diffs will need to be generated again when voting on the segment matches.
   These votes use all_diffs in combination with the best-fit parameters to
   dole out votes towards matches.

# An example skeletal generator routine for diffs:
def _diffs_in_batches(chunk_size=whatever):
    tseg1 = ...
    tseg2 = ...
    for s1chunk in np.array_split(tseg1, nchunks, axis=0):
        tdiffs = s1chunk[:, None] - tseg2[None, :]
        tdiffs[(tdiffs >  180.0)] -= 360.0
        tdiffs[(tdiffs < -180.0)] += 360.0
        yield tdiffs

def _segments_from_batch_diffs(self, pars, tols):
    rows_seen = 0
    results = []
    for dbatch in self._diffs_in_batches():
        match_coo = np.all(np.abs(dbatch - pars) < tols, axis=-1).nonzero()
        match_coo += np.array([rows_seen, 0.0])
        results.append(match_coo)
        rows_seen += dbatch.shape[0]
    return np.concatenate(results)  # or maybe vstack?

# -----------------------------------------------------------------------

